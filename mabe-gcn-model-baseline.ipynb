{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MABe CTR-GCN Baseline (GCN-Only)\n",
    "\n",
    "This notebook contains a standalone, GCN-only baseline for the\n",
    "MABe mouse behavior detection challenge. It supports three modes:\n",
    "\n",
    "- `dev`: quick debugging on small subsets.\n",
    "- `validate`: external training/validation workflows.\n",
    "- `submit`: inference-only on the Kaggle test set using pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Global mode selector for this notebook:\n",
    "#   - 'dev'      : quick tests / debugging\n",
    "#   - 'validate' : external training + validation workflows\n",
    "#   - 'submit'   : inference-only on Kaggle test set\n",
    "# ------------------------------------------------------------------\n",
    "RUN_MODE = \"submit\"  # change to 'dev' or 'validate' when running locally\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}, RUN_MODE={RUN_MODE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CTR-GCN Configuration, Models, and Core Pipeline\n",
    "\n",
    "This section defines the `CTRGCNConfig`, joint ordering, windowing,\n",
    "CTR-GCN model variants (one-/two-/four-stream), and training/inference\n",
    "helpers (`train_ctr_gcn_models`, `load_ctr_gcn_models`, `submit_ctr_gcn`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook copied from Amrosm on Kaggle from the publicly available code for the competition\n",
    "'''\n",
    "# MABe Challenge - Social Action Recognition in Mice: Nearest neighbors\n",
    "\n",
    "This is the original notebook for social action recognition with nearest neighbors. \n",
    "I've tried to explain what the code does—feel free to ask questions.\n",
    "\n",
    "The notebook shows how to overcome the five challenges of this competition:\n",
    "1. Modeling for variable-size sets of mice\n",
    "2. Multiclass prediction with missing labels\n",
    "3. Transforming coordinates to an invariant representation\n",
    "4. A dataset that doesn't fit into memory\n",
    "5. Modeling for variable sets of body parts\n",
    "\n",
    "The title of the notebook mentions *Nearest Neighbors* \n",
    "because in earlier versions I used nearest neighbors classification, \n",
    "an algorithm which doesn't need a lot of tuning. \n",
    "The current version uses LightGBM, and maybe I'll ensemble the two later.\n",
    "\n",
    "References\n",
    "- Competition: [MABe Challenge - Social Action Recognition in Mice](https://www.kaggle.com/competitions/MABe-mouse-behavior-detection)\n",
    "- [MABe EDA which makes sense ⭐️⭐️⭐️⭐️⭐️](https://www.kaggle.com/code/ambrosm/mabe-eda-which-makes-sense)\n",
    "- [MABe Validated baseline without machine learning](https://www.kaggle.com/code/ambrosm/mabe-validated-baseline-without-machine-learning)\n",
    "\n",
    "This notebook can be run in validate or submission mode. \n",
    "If you look at other saved versions of this notebook, you'll see both modes. \n",
    "You can switch between the modes by setting the variable `validate_or_submit`:\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import lightgbm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator, clone\n",
    "from sklearn.model_selection import cross_val_predict, GroupKFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CTRGCNConfig:\n",
    "    \"\"\"\n",
    "    Configuration for controlling the CTR-GCN training pipeline.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mode : str\n",
    "        One of {\"dev\", \"validate\", \"submit\"}.\n",
    "        - \"dev\": train on a very small subset (quick tests)\n",
    "        - \"validate\": cross-validation on the full dataset\n",
    "        - \"submit\": full training on everything for submission\n",
    "\n",
    "    max_videos : int | None\n",
    "        If not None, limit the number of videos processed during training.\n",
    "\n",
    "    max_batches : int | None\n",
    "        If not None, limit how many batches from generate_mouse_data() are processed.\n",
    "\n",
    "    max_windows : int | None\n",
    "        If not None, limit how many sliding windows per batch we convert to tensors.\n",
    "\n",
    "    use_delta : bool\n",
    "        Whether to compute Δx, Δy velocity channels.\n",
    "\n",
    "    two_stream : bool\n",
    "        If True: return coords and delta as two separate tensors for a two-stream CTR-GCN.\n",
    "        If False: merge coords + delta into a single stream with extra channels.\n",
    "\n",
    "    in_channels_coords : int\n",
    "        Coordinate channels (x, y).\n",
    "\n",
    "    in_channels_delta : int\n",
    "        Δx, Δy channels.\n",
    "\n",
    "    in_channels_single_stream : int\n",
    "        Combined coords (x, y) + delta (Δx, Δy) for single-stream input.\n",
    "\n",
    "    use_bone : bool\n",
    "        Whether to compute bone vectors.\n",
    "\n",
    "    use_bone_delta : bool\n",
    "        Whether to compute bone deltas.\n",
    "\n",
    "    stream_mode : str\n",
    "        One of {\"one\", \"two\", \"four\"}; controls stream splitting for CTR-GCN.\n",
    "\n",
    "    in_channels_streamA : int\n",
    "        Channels for stream A (coords + bone) in two-stream mode.\n",
    "\n",
    "    in_channels_streamB : int\n",
    "        Channels for stream B (delta + bone_delta) in two-stream mode.\n",
    "\n",
    "    in_channels_coords_only : int\n",
    "        Channels for coords-only stream in four-stream mode.\n",
    "\n",
    "    in_channels_delta_only : int\n",
    "        Channels for delta-only stream in four-stream mode.\n",
    "\n",
    "    in_channels_bone_only : int\n",
    "        Channels for bone-only stream in four-stream mode.\n",
    "\n",
    "    in_channels_bone_delta_only : int\n",
    "        Channels for bone-delta-only stream in four-stream mode.\n",
    "    \"\"\"\n",
    "\n",
    "    mode: str = \"dev\"\n",
    "    max_videos: int | None = 3\n",
    "    max_batches: int | None = 10\n",
    "    max_windows: int | None = 50\n",
    "    use_delta: bool = True\n",
    "    two_stream: bool = False\n",
    "    in_channels_coords: int = 2\n",
    "    in_channels_delta: int = 2\n",
    "    in_channels_single_stream: int = 4\n",
    "    use_bone: bool = True\n",
    "    use_bone_delta: bool = True\n",
    "    stream_mode: str = \"one\"\n",
    "    in_channels_streamA: int = 4\n",
    "    in_channels_streamB: int = 4\n",
    "    in_channels_coords_only: int = 2\n",
    "    in_channels_delta_only: int = 2\n",
    "    in_channels_bone_only: int = 2\n",
    "    in_channels_bone_delta_only: int = 2\n",
    "\n",
    "# Functions requiring updates for stream_mode and bone features:\n",
    "# - prepare_ctr_gcn_input\n",
    "# - train_ctr_gcn_models\n",
    "# - CTRGCNTwoStream (extend input channels)\n",
    "# - (new) CTRGCNFourStream\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CTR-GCN: Master anatomical ordering for all mouse body parts\n",
    "# ------------------------------------------------------------\n",
    "MASTER_MOUSE_JOINT_ORDER = [\n",
    "    \"nose\",\n",
    "    \"head\",\n",
    "\n",
    "    \"headpiece_topfrontleft\",\n",
    "    \"headpiece_topfrontright\",\n",
    "    \"headpiece_topbackleft\",\n",
    "    \"headpiece_topbackright\",\n",
    "    \"headpiece_bottomfrontleft\",\n",
    "    \"headpiece_bottomfrontright\",\n",
    "    \"headpiece_bottombackleft\",\n",
    "    \"headpiece_bottombackright\",\n",
    "\n",
    "    \"ear_left\",\n",
    "    \"ear_right\",\n",
    "\n",
    "    \"neck\",\n",
    "\n",
    "    \"forepaw_left\",\n",
    "    \"forepaw_right\",\n",
    "\n",
    "    \"body_center\",\n",
    "    \"lateral_left\",\n",
    "    \"lateral_right\",\n",
    "\n",
    "    \"spine_1\",\n",
    "    \"spine_2\",\n",
    "\n",
    "    \"hip_left\",\n",
    "    \"hip_right\",\n",
    "\n",
    "    \"hindpaw_left\",\n",
    "    \"hindpaw_right\",\n",
    "\n",
    "    \"tail_base\",\n",
    "    \"tail_middle_1\",\n",
    "    \"tail_middle_2\",\n",
    "    \"tail_midpoint\",\n",
    "    \"tail_tip\",\n",
    "]\n",
    "# ------------------------------------------------------------\n",
    "# CTR-GCN: Ordering joints + building adjacency matrix\n",
    "# ------------------------------------------------------------\n",
    "def get_ordered_joints_and_adjacency(body_parts_tracked):\n",
    "    \"\"\"\n",
    "    Sorts the subset of tracked body parts according to MASTER_MOUSE_JOINT_ORDER\n",
    "    and builds a simple chain adjacency matrix (i <-> i+1).\n",
    "    \"\"\"\n",
    "    ordered_joints = [\n",
    "        bp for bp in MASTER_MOUSE_JOINT_ORDER\n",
    "        if bp in body_parts_tracked\n",
    "    ]\n",
    "\n",
    "    V = len(ordered_joints)\n",
    "    adjacency = np.zeros((V, V), dtype=np.float32)\n",
    "\n",
    "    for i in range(V - 1):\n",
    "        adjacency[i, i + 1] = 1.0\n",
    "        adjacency[i + 1, i] = 1.0\n",
    "\n",
    "    return ordered_joints, adjacency\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CTR-GCN: Sliding window extraction (window=90, stride=30)\n",
    "# ------------------------------------------------------------\n",
    "def create_sliding_windows_90_30(single_mouse_df):\n",
    "    \"\"\"\n",
    "    Takes a continuous single-mouse coordinate DataFrame and yields\n",
    "    windows of length 90 with stride 30, preserving frame indices.\n",
    "    \"\"\"\n",
    "    WINDOW = 90\n",
    "    STRIDE = 30\n",
    "\n",
    "    n_frames = len(single_mouse_df)\n",
    "    frames = single_mouse_df.index.to_numpy()\n",
    "\n",
    "    for start in range(0, n_frames - WINDOW + 1, STRIDE):\n",
    "        end = start + WINDOW\n",
    "        window_df = single_mouse_df.iloc[start:end]\n",
    "        frame_indices = frames[start:end]\n",
    "        yield window_df, frame_indices\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CTR-GCN: Build tensors for spatio-temporal model input\n",
    "# ------------------------------------------------------------\n",
    "def prepare_ctr_gcn_input(single_mouse_df, ordered_joints, config: CTRGCNConfig | None = None):\n",
    "    \"\"\"\n",
    "    Convert a sliding-windowed single-mouse DataFrame into\n",
    "    CTR-GCN input tensors of shape (N, 2, V, 90).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    single_mouse_df : DataFrame\n",
    "        Raw continuous coordinate data for a single mouse.\n",
    "        Columns are a two-level MultiIndex: (bodypart, xy).\n",
    "\n",
    "    ordered_joints : list[str]\n",
    "        Anatomically ordered joints for this model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensors : torch.Tensor of shape (N, 2, V, 90)\n",
    "    frame_ranges : list of np.ndarray\n",
    "        Each element is the frame indices for that window.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The optional config.max_windows limit is useful for dev/testing to cap work.\n",
    "    The optional config.use_delta/config.two_stream settings enable velocity and two-stream output for dev/validate/submit.\n",
    "    \"\"\"\n",
    "    mode = \"one\"\n",
    "    if config is not None:\n",
    "        mode = config.stream_mode if hasattr(config, \"stream_mode\") else \"one\"\n",
    "        if getattr(config, \"two_stream\", False) and mode == \"one\":\n",
    "            mode = \"two\"\n",
    "    V = len(ordered_joints)\n",
    "    frame_ranges = []\n",
    "    window_count = 0\n",
    "    if config is None or mode == \"one\":\n",
    "        window_tensors = []\n",
    "    elif mode == \"two\":\n",
    "        streamA_tensors = []\n",
    "        streamB_tensors = []\n",
    "    else:  # four\n",
    "        coords_tensors = []\n",
    "        delta_tensors = []\n",
    "        bone_tensors = []\n",
    "        bone_delta_tensors = []\n",
    "\n",
    "    for window_df, frame_indices in create_sliding_windows_90_30(single_mouse_df):\n",
    "        if config is not None and config.max_windows is not None and window_count >= config.max_windows:\n",
    "            break\n",
    "        if len(window_df) != 90:\n",
    "            # Skip incomplete windows to keep tensor shape consistent.\n",
    "            continue\n",
    "\n",
    "        window_np = np.full((2, V, 90), np.nan, dtype=np.float32)\n",
    "        for j, bp in enumerate(ordered_joints):\n",
    "            try:\n",
    "                coords = window_df[bp]\n",
    "                window_np[0, j, :] = coords['x'].to_numpy(dtype=np.float32, copy=False)\n",
    "                window_np[1, j, :] = coords['y'].to_numpy(dtype=np.float32, copy=False)\n",
    "            except KeyError:\n",
    "                # Missing bodypart columns remain NaN.\n",
    "                continue\n",
    "\n",
    "        if config is None:\n",
    "            window_tensors.append(torch.from_numpy(window_np))\n",
    "            frame_ranges.append(frame_indices)\n",
    "            window_count += 1\n",
    "            continue\n",
    "\n",
    "        # Compute bone vectors (pad last with zeros)\n",
    "        bone = np.zeros_like(window_np)\n",
    "        for j in range(V - 1):\n",
    "            bone[:, j, :] = window_np[:, j + 1, :] - window_np[:, j, :]\n",
    "\n",
    "        # Step 1: mean-centering\n",
    "        mean_val = np.nanmean(window_np, axis=2, keepdims=True)\n",
    "        window_np = window_np - mean_val\n",
    "        bone = bone - mean_val\n",
    "\n",
    "        # Step 2: root-joint normalization\n",
    "        anchor_name = None\n",
    "        if \"body_center\" in ordered_joints:\n",
    "            anchor_name = \"body_center\"\n",
    "        elif \"neck\" in ordered_joints:\n",
    "            anchor_name = \"neck\"\n",
    "        if anchor_name is not None:\n",
    "            anchor_idx = ordered_joints.index(anchor_name)\n",
    "            anchor = window_np[:, anchor_idx:anchor_idx+1, :]\n",
    "            window_np = window_np - anchor\n",
    "            bone = bone - anchor\n",
    "\n",
    "        # Step 3: scale normalization\n",
    "        scale = np.nanstd(window_np)\n",
    "        if scale > 0:\n",
    "            window_np = window_np / scale\n",
    "            bone = bone / scale\n",
    "\n",
    "        # Deltas\n",
    "        if config.use_delta:\n",
    "            delta = window_np[:, :, 1:] - window_np[:, :, :-1]\n",
    "            delta = np.concatenate([np.zeros_like(delta[:, :, :1]), delta], axis=2)\n",
    "        else:\n",
    "            delta = np.zeros_like(window_np)\n",
    "\n",
    "        if config.use_bone:\n",
    "            bone_curr = bone\n",
    "        else:\n",
    "            bone_curr = np.zeros_like(window_np)\n",
    "\n",
    "        if config.use_bone_delta:\n",
    "            bone_delta = bone_curr[:, :, 1:] - bone_curr[:, :, :-1]\n",
    "            bone_delta = np.concatenate([np.zeros_like(bone_delta[:, :, :1]), bone_delta], axis=2)\n",
    "        else:\n",
    "            bone_delta = np.zeros_like(window_np)\n",
    "\n",
    "        if mode == \"one\":\n",
    "            merged = np.concatenate([window_np, delta, bone_curr, bone_delta], axis=0)\n",
    "            window_tensors.append(torch.from_numpy(merged.astype(np.float32)))\n",
    "        elif mode == \"two\":\n",
    "            streamA = np.concatenate([window_np, bone_curr], axis=0)\n",
    "            streamB = np.concatenate([delta, bone_delta], axis=0)\n",
    "            streamA_tensors.append(torch.from_numpy(streamA.astype(np.float32)))\n",
    "            streamB_tensors.append(torch.from_numpy(streamB.astype(np.float32)))\n",
    "        else:  # four\n",
    "            coords_tensors.append(torch.from_numpy(window_np.astype(np.float32)))\n",
    "            delta_tensors.append(torch.from_numpy(delta.astype(np.float32)))\n",
    "            bone_tensors.append(torch.from_numpy(bone_curr.astype(np.float32)))\n",
    "            bone_delta_tensors.append(torch.from_numpy(bone_delta.astype(np.float32)))\n",
    "\n",
    "        frame_ranges.append(frame_indices)\n",
    "        window_count += 1\n",
    "\n",
    "    if config is None or mode == \"one\":\n",
    "        if len(window_tensors) == 0:\n",
    "            channels = 2 if config is None else config.in_channels_single_stream\n",
    "            return torch.empty((0, channels, V, 90)), frame_ranges\n",
    "        return torch.stack(window_tensors, dim=0), frame_ranges\n",
    "    if mode == \"two\":\n",
    "        return streamA_tensors, streamB_tensors, frame_ranges\n",
    "    return coords_tensors, delta_tensors, bone_tensors, bone_delta_tensors, frame_ranges\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CTR-GCN: Minimal spatio-temporal GCN model (single-mouse)\n",
    "# ------------------------------------------------------------\n",
    "def _normalize_adjacency_chain(adjacency: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add self-loops and row-normalize a simple chain adjacency matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency : np.ndarray of shape (V, V)\n",
    "        Symmetric adjacency matrix for the joints graph.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A_norm : np.ndarray of shape (V, V)\n",
    "        Row-normalized adjacency with self-loops.\n",
    "    \"\"\"\n",
    "    V = adjacency.shape[0]\n",
    "    A = adjacency.astype(np.float32).copy()\n",
    "    # Add self-loops\n",
    "    A += np.eye(V, dtype=np.float32)\n",
    "    # Row-normalize\n",
    "    row_sum = A.sum(axis=1, keepdims=True)\n",
    "    row_sum[row_sum == 0.0] = 1.0\n",
    "    A_norm = A / row_sum\n",
    "    return A_norm\n",
    "\n",
    "\n",
    "class GraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple graph convolution operating on (N, C, V, T) tensors.\n",
    "\n",
    "    Given an adjacency matrix A (V, V), this layer first aggregates\n",
    "    information from neighboring joints via A, then applies a 1x1\n",
    "    convolution over the channel dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, adjacency: np.ndarray):\n",
    "        super().__init__()\n",
    "        A_norm = _normalize_adjacency_chain(adjacency)\n",
    "        self.register_buffer(\"A\", torch.from_numpy(A_norm))  # shape (V, V)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (N, C, V, T)\n",
    "        returns: (N, C_out, V, T)\n",
    "        \"\"\"\n",
    "        # Aggregate neighbor information along the joint dimension\n",
    "        # Using einsum: (N, C, V, T) x (V, V) -> (N, C, V, T)\n",
    "        x = torch.einsum(\"ncvT,vw->ncwT\", x, self.A)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class STBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-temporal block:\n",
    "    - Graph convolution over joints\n",
    "    - Temporal convolution over frames\n",
    "    - Residual connection (if shapes match)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        adjacency: np.ndarray,\n",
    "        stride: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gcn = GraphConv(in_channels, out_channels, adjacency)\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(1, 3),\n",
    "                padding=(0, 1),\n",
    "                stride=(1, stride),\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        if (in_channels != out_channels) or (stride != 1):\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=(1, stride),\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (N, C_in, V, T)\n",
    "        returns: (N, C_out, V, T_out)\n",
    "        \"\"\"\n",
    "        res = self.residual(x)\n",
    "        x = self.gcn(x)\n",
    "        x = self.tcn(x)\n",
    "        x = x + res\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CTRGCNMinimal(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal CTR-GCN-style model for single-mouse behavior classification.\n",
    "\n",
    "    This model:\n",
    "    - Expects input of shape (N, C, V, T) where:\n",
    "        N = batch size\n",
    "        C = number of channels (2 for x/y coordinates)\n",
    "        V = number of joints (len(ordered_joints))\n",
    "        T = number of frames (e.g., 90)\n",
    "    - Uses a simple chain adjacency matrix per body-part set.\n",
    "    - Outputs logits of shape (N, num_classes) for binary or multi-label tasks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_classes: int,\n",
    "        adjacency: np.ndarray,\n",
    "        base_channels: int = 64,\n",
    "        num_blocks: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.V = adjacency.shape[0]\n",
    "\n",
    "        channels = [base_channels] * num_blocks\n",
    "        blocks = []\n",
    "        last_c = in_channels\n",
    "        for i, out_c in enumerate(channels):\n",
    "            # No temporal downsampling for now (stride = 1)\n",
    "            blocks.append(\n",
    "                STBlock(\n",
    "                    in_channels=last_c,\n",
    "                    out_channels=out_c,\n",
    "                    adjacency=adjacency,\n",
    "                    stride=1,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "            last_c = out_c\n",
    "\n",
    "        self.st_blocks = nn.ModuleList(blocks)\n",
    "        self.fc = nn.Linear(last_c, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (N, C, V, T)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Output tensor of shape (N, num_classes).\n",
    "        \"\"\"\n",
    "        # Ensure correct shape\n",
    "        assert x.ndim == 4, f\"Expected (N, C, V, T), got {x.shape}\"\n",
    "\n",
    "        out = x\n",
    "        for block in self.st_blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # Global average pooling over joints (V) and time (T)\n",
    "        out = out.mean(dim=(-2, -1))  # (N, C_out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CTRGCNTwoStream(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-stream CTR-GCN.\n",
    "\n",
    "    - Stream A processes coordinate channels (x, y or similar).\n",
    "    - Stream B processes delta/velocity channels (Δx, Δy).\n",
    "    - Features are fused via summation and fed to a final classifier head.\n",
    "    - Use with CTRGCNConfig.two_stream=True when supplying separate coord/delta inputs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        adjacency: np.ndarray,\n",
    "        in_channels_coords: int = 2,\n",
    "        in_channels_delta: int = 2,\n",
    "        base_channels: int = 64,\n",
    "        num_blocks: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stream_coords = CTRGCNMinimal(\n",
    "            in_channels=in_channels_coords,\n",
    "            num_classes=base_channels,\n",
    "            adjacency=adjacency,\n",
    "            base_channels=base_channels,\n",
    "            num_blocks=num_blocks,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.stream_delta = CTRGCNMinimal(\n",
    "            in_channels=in_channels_delta,\n",
    "            num_classes=base_channels,\n",
    "            adjacency=adjacency,\n",
    "            base_channels=base_channels,\n",
    "            num_blocks=num_blocks,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(base_channels, 1)\n",
    "\n",
    "    def forward(self, coords_x: torch.Tensor, delta_x: torch.Tensor) -> torch.Tensor:\n",
    "        feat_A = self.stream_coords(coords_x)\n",
    "        feat_B = self.stream_delta(delta_x)\n",
    "        fused = feat_A + feat_B\n",
    "        logits = self.fc(fused)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CTRGCNFourStream(nn.Module):\n",
    "    \"\"\"\n",
    "    Four-stream CTR-GCN:\n",
    "      Stream 1: coords\n",
    "      Stream 2: deltas\n",
    "      Stream 3: bones\n",
    "      Stream 4: bone_deltas\n",
    "    Fuses all four via elementwise sum, followed by a linear classifier.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        adjacency,\n",
    "        base_channels=64,\n",
    "        dropout=0.1,\n",
    "        num_blocks=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stream_coords = CTRGCNMinimal(2, base_channels, adjacency, base_channels, num_blocks, dropout)\n",
    "        self.stream_delta = CTRGCNMinimal(2, base_channels, adjacency, base_channels, num_blocks, dropout)\n",
    "        self.stream_bone = CTRGCNMinimal(2, base_channels, adjacency, base_channels, num_blocks, dropout)\n",
    "        self.stream_bone_delta = CTRGCNMinimal(2, base_channels, adjacency, base_channels, num_blocks, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(base_channels, 1)\n",
    "\n",
    "    def forward(self, coords_x, delta_x, bone_x, bone_delta_x):\n",
    "        f1 = self.stream_coords(coords_x)\n",
    "        f2 = self.stream_delta(delta_x)\n",
    "        f3 = self.stream_bone(bone_x)\n",
    "        f4 = self.stream_bone_delta(bone_delta_x)\n",
    "        fused = f1 + f2 + f3 + f4\n",
    "        return self.fc(fused)\n",
    "\n",
    "\n",
    "def train_ctr_gcn_models(\n",
    "    batches,\n",
    "    ordered_joints,\n",
    "    adjacency,\n",
    "    config: CTRGCNConfig,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train one CTR-GCN binary classifier per action using sliding-window inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batches : list of tuples\n",
    "        Each item is (data_df, meta_df, label_df) for a single-mouse batch.\n",
    "        data_df: raw coordinate data (pvid)\n",
    "        meta_df: video/agent/target/frame metadata\n",
    "        label_df: per-frame binary labels for each action\n",
    "\n",
    "    ordered_joints : list[str]\n",
    "        Anatomically ordered joints.\n",
    "\n",
    "    adjacency : np.ndarray\n",
    "        V x V adjacency matrix for CTR-GCN.\n",
    "\n",
    "    config : CTRGCNConfig\n",
    "        Controls max_windows, max_batches, and dev/validate/submit modes.\n",
    "\n",
    "    device : str\n",
    "        \"cpu\", \"cuda\", or \"mps\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_dict : dict[str, CTRGCNMinimal]\n",
    "        A dictionary mapping action → trained CTR-GCN model.\n",
    "    \"\"\"\n",
    "    model_dict: dict[str, CTRGCNMinimal] = {}\n",
    "    y_dict: dict[str, list] = {}\n",
    "    mode = config.stream_mode if hasattr(config, \"stream_mode\") else \"one\"\n",
    "    if getattr(config, \"two_stream\", False) and mode == \"one\":\n",
    "        mode = \"two\"\n",
    "    if mode == \"one\":\n",
    "        X_windows: list[torch.Tensor] = []\n",
    "    elif mode == \"two\":\n",
    "        streamA_windows: list[torch.Tensor] = []\n",
    "        streamB_windows: list[torch.Tensor] = []\n",
    "    else:\n",
    "        coords_windows: list[torch.Tensor] = []\n",
    "        delta_windows: list[torch.Tensor] = []\n",
    "        bone_windows: list[torch.Tensor] = []\n",
    "        bone_delta_windows: list[torch.Tensor] = []\n",
    "\n",
    "    batch_count = 0\n",
    "    for data_df, meta_df, label_df in batches:\n",
    "        if config.max_batches is not None and batch_count >= config.max_batches:\n",
    "            break\n",
    "\n",
    "        if mode == \"one\":\n",
    "            window_tensor, frame_ranges = prepare_ctr_gcn_input(data_df, ordered_joints, config)\n",
    "            if window_tensor.shape[0] == 0:\n",
    "                batch_count += 1\n",
    "                continue\n",
    "        elif mode == \"two\":\n",
    "            streamA_list, streamB_list, frame_ranges = prepare_ctr_gcn_input(data_df, ordered_joints, config)\n",
    "            if len(streamA_list) == 0:\n",
    "                batch_count += 1\n",
    "                continue\n",
    "        else:\n",
    "            coords_list, delta_list, bone_list, bone_delta_list, frame_ranges = prepare_ctr_gcn_input(data_df, ordered_joints, config)\n",
    "            if len(coords_list) == 0:\n",
    "                batch_count += 1\n",
    "                continue\n",
    "\n",
    "        # Ensure label tracking per action\n",
    "        for action in label_df.columns:\n",
    "            if action not in y_dict:\n",
    "                y_dict[action] = []\n",
    "\n",
    "        for i, frame_range in enumerate(frame_ranges):\n",
    "            center_frame = frame_range[len(frame_range) // 2]\n",
    "            if center_frame not in label_df.index:\n",
    "                continue\n",
    "            label_row = label_df.loc[center_frame]\n",
    "            if mode == \"one\":\n",
    "                X_windows.append(window_tensor[i])\n",
    "            elif mode == \"two\":\n",
    "                streamA_windows.append(streamA_list[i])\n",
    "                streamB_windows.append(streamB_list[i])\n",
    "            else:\n",
    "                coords_windows.append(coords_list[i])\n",
    "                delta_windows.append(delta_list[i])\n",
    "                bone_windows.append(bone_list[i])\n",
    "                bone_delta_windows.append(bone_delta_list[i])\n",
    "            for action in y_dict.keys():\n",
    "                val = label_row[action] if action in label_row else np.nan\n",
    "                y_dict[action].append(val)\n",
    "\n",
    "        batch_count += 1\n",
    "\n",
    "    if mode == \"one\" and len(X_windows) == 0:\n",
    "        return model_dict\n",
    "    if mode == \"two\" and len(streamA_windows) == 0:\n",
    "        return model_dict\n",
    "    if mode == \"four\" and len(coords_windows) == 0:\n",
    "        return model_dict\n",
    "\n",
    "    batch_size = 16\n",
    "    if config.mode == \"dev\":\n",
    "        epochs = 1\n",
    "    elif config.mode == \"validate\":\n",
    "        epochs = 3\n",
    "    else:\n",
    "        epochs = 5\n",
    "\n",
    "    if mode == \"one\":\n",
    "        X = torch.stack(X_windows, dim=0).to(device)\n",
    "        in_channels_single = X.shape[1]\n",
    "    elif mode == \"two\":\n",
    "        X_streamA = torch.stack(streamA_windows, dim=0).to(device)\n",
    "        X_streamB = torch.stack(streamB_windows, dim=0).to(device)\n",
    "    else:\n",
    "        X_coords = torch.stack(coords_windows, dim=0).to(device)\n",
    "        X_delta = torch.stack(delta_windows, dim=0).to(device)\n",
    "        X_bone = torch.stack(bone_windows, dim=0).to(device)\n",
    "        X_bone_delta = torch.stack(bone_delta_windows, dim=0).to(device)\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    for action, labels in y_dict.items():\n",
    "        y_tensor = torch.tensor(labels, dtype=torch.float32, device=device)\n",
    "        mask = ~torch.isnan(y_tensor)\n",
    "        if mask.sum().item() == 0:\n",
    "            continue\n",
    "        y_action = y_tensor[mask].unsqueeze(1)\n",
    "\n",
    "        if mode == \"one\":\n",
    "            X_action = X[mask]\n",
    "            model = CTRGCNMinimal(in_channels=in_channels_single, num_classes=1, adjacency=adjacency).to(device)\n",
    "        elif mode == \"two\":\n",
    "            X_action_A = X_streamA[mask]\n",
    "            X_action_B = X_streamB[mask]\n",
    "            model = CTRGCNTwoStream(\n",
    "                adjacency=adjacency,\n",
    "                in_channels_coords=X_action_A.shape[1],\n",
    "                in_channels_delta=X_action_B.shape[1],\n",
    "            ).to(device)\n",
    "        else:\n",
    "            X_action_coords = X_coords[mask]\n",
    "            X_action_delta = X_delta[mask]\n",
    "            X_action_bone = X_bone[mask]\n",
    "            X_action_bone_delta = X_bone_delta[mask]\n",
    "            model = CTRGCNFourStream(adjacency=adjacency).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            model.train()\n",
    "            for start in range(0, len(y_action), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_y = y_action[start:end]\n",
    "                if mode == \"one\":\n",
    "                    batch_x = X_action[start:end]\n",
    "                    logits = model(batch_x)\n",
    "                elif mode == \"two\":\n",
    "                    batch_x_A = X_action_A[start:end]\n",
    "                    batch_x_B = X_action_B[start:end]\n",
    "                    logits = model(batch_x_A, batch_x_B)\n",
    "                else:\n",
    "                    batch_x_coords = X_action_coords[start:end]\n",
    "                    batch_x_delta = X_action_delta[start:end]\n",
    "                    batch_x_bone = X_action_bone[start:end]\n",
    "                    batch_x_bone_delta = X_action_bone_delta[start:end]\n",
    "                    logits = model(batch_x_coords, batch_x_delta, batch_x_bone, batch_x_bone_delta)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(logits, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model_dict[action] = model\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "validate_or_submit = 'stresstest' # 'validate' or 'submit' or 'stresstest'\n",
    "verbose = True\n",
    "cwd = Path.cwd()\n",
    "\n",
    "class TrainOnSubsetClassifier(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\"Fit estimator to a subset of the training data.\"\"\"\n",
    "    def __init__(self, estimator, n_samples):\n",
    "        self.estimator = estimator\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        downsample = len(X) // self.n_samples\n",
    "        downsample = max(downsample, 1)\n",
    "        self.estimator.fit(np.array(X, copy=False)[::downsample],\n",
    "                           np.array(y, copy=False)[::downsample])\n",
    "        self.classes_ = self.estimator.classes_\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if len(self.classes_) == 1:\n",
    "            return np.full((len(X), 1), 1.0)\n",
    "        probs = self.estimator.predict_proba(np.array(X))\n",
    "        return probs\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(np.array(X))\n",
    "    \n",
    "\"\"\"F Beta customized for the data format of the MABe challenge.\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n",
    "    label_frames: defaultdict[str, set[int]] = defaultdict(set) # key is video/agent/target/action from solution\n",
    "    prediction_frames: defaultdict[str, set[int]] = defaultdict(set) # key is video/agent/target/action from submission\n",
    "\n",
    "    for row in lab_solution.to_dicts():\n",
    "        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n",
    "\n",
    "    for video in lab_solution['video_id'].unique():\n",
    "        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()  # ty: ignore\n",
    "        active_labels: set[str] = set(json.loads(active_labels)) # set of agent,target,action from solution\n",
    "        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set) # key is agent,target from submission\n",
    "\n",
    "        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts(): # every submission row is converted to a dict\n",
    "            # Since the labels are sparse, we can't evaluate prediction keys not in the active labels.\n",
    "            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n",
    "                # print(f'ignoring {video}', ','.join([str(row['agent_id']), str(row['target_id']), row['action']]), active_labels)\n",
    "                continue # these submission rows are ignored\n",
    "           \n",
    "            new_frames = set(range(row['start_frame'], row['stop_frame']))\n",
    "            # Ignore truly redundant predictions.\n",
    "            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n",
    "            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n",
    "            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n",
    "                # A single agent can have multiple targets per frame (ex: evading all other mice) but only one action per target per frame.\n",
    "                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n",
    "            prediction_frames[row['prediction_key']].update(new_frames)\n",
    "            predicted_mouse_pairs[prediction_pair].update(new_frames)\n",
    "\n",
    "    tps = defaultdict(int) # key is action\n",
    "    fns = defaultdict(int) # key is action\n",
    "    fps = defaultdict(int) # key is action\n",
    "    for key, pred_frames in prediction_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        matched_label_frames = label_frames[key]\n",
    "        tps[action] += len(pred_frames.intersection(matched_label_frames))\n",
    "        fns[action] += len(matched_label_frames.difference(pred_frames))\n",
    "        fps[action] += len(pred_frames.difference(matched_label_frames))\n",
    "\n",
    "    distinct_actions = set()\n",
    "    for key, frames in label_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        distinct_actions.add(action)\n",
    "        if key not in prediction_frames:\n",
    "            fns[action] += len(frames)\n",
    "\n",
    "    action_f1s = []\n",
    "    for action in distinct_actions:\n",
    "        # print(f\"{tps[action]:8} {fns[action]:8} {fps[action]:8}\")\n",
    "        if tps[action] + fns[action] + fps[action] == 0:\n",
    "            action_f1s.append(0)\n",
    "        else:\n",
    "            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n",
    "    return sum(action_f1s) / len(action_f1s)\n",
    "\n",
    "\n",
    "def mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    Doctests:\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10},\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 0, 'stop_frame': 10}, # Wrong action\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    0.0\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
    "    ... ])\n",
    "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
    "    '0.500000000000'\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
    "    ... ])\n",
    "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
    "    '0.250000000000'\n",
    "\n",
    "    >>> # Overlapping solution events, one prediction matching both.\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 10, 'stop_frame': 20, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 20},\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 30, 'stop_frame': 40, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 40},\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    0.6666666666666666\n",
    "    \"\"\"\n",
    "    if len(solution) == 0 or len(submission) == 0:\n",
    "        raise ValueError('Missing solution or submission data')\n",
    "\n",
    "    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "\n",
    "    for col in expected_cols:\n",
    "        if col not in solution.columns:\n",
    "            raise ValueError(f'Solution is missing column {col}')\n",
    "        if col not in submission.columns:\n",
    "            raise ValueError(f'Submission is missing column {col}')\n",
    "\n",
    "    solution: pl.DataFrame = pl.DataFrame(solution)\n",
    "    submission: pl.DataFrame = pl.DataFrame(submission)\n",
    "    assert (solution['start_frame'] <= solution['stop_frame']).all()\n",
    "    assert (submission['start_frame'] <= submission['stop_frame']).all()\n",
    "    solution_videos = set(solution['video_id'].unique())\n",
    "    # Need to align based on video IDs as we can't rely on the row IDs for handling public/private splits.\n",
    "    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n",
    "\n",
    "    solution = solution.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('label_key'),\n",
    "    )\n",
    "    submission = submission.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('prediction_key'),\n",
    "    )\n",
    "\n",
    "    lab_scores = []\n",
    "    for lab in solution['lab_id'].unique():\n",
    "        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n",
    "        lab_videos = set(lab_solution['video_id'].unique())\n",
    "        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n",
    "        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n",
    "\n",
    "    return sum(lab_scores) / len(lab_scores)\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    F1 score for the MABe Challenge\n",
    "    \"\"\"\n",
    "    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    return mouse_fbeta(solution, submission, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading, Batching, and Competition Metric\n",
    "\n",
    "This section defines the MABe data loading logic (`train`, `test`),\n",
    "batch generator (`generate_mouse_data`), and the official competition\n",
    "metric implementation (`mouse_fbeta`, `score`). It also includes\n",
    "`predict_multiclass`, which converts per-frame binary probabilities\n",
    "        into action segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(cwd / 'Data' / 'train.csv')\n",
    "train['n_mice'] = 4 - train[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].isna().sum(axis=1)\n",
    "train_without_mabe22 = train.query(\"~ lab_id.str.startswith('MABe22_')\")\n",
    "\n",
    "test = pd.read_csv(cwd / 'Data' / 'test.csv')\n",
    "\n",
    "# labs = list(np.unique(train.lab_id))\n",
    "\n",
    "body_parts_tracked_list = list(np.unique(train.body_parts_tracked))\n",
    "\n",
    "# behaviors = list(train.behaviors_labeled.drop_duplicates().dropna())\n",
    "# behaviors = sorted(list({b.replace(\"'\", \"\") for bb in behaviors for b in json.loads(bb)}))\n",
    "# behaviors = [b.split(',') for b in behaviors]\n",
    "# behaviors = pd.DataFrame(behaviors, columns=['agent', 'target', 'action'])\n",
    "\n",
    "\n",
    "def create_solution_df(dataset):\n",
    "    \"\"\"Create the solution dataframe for validating out-of-fold predictions.\n",
    "\n",
    "    From https://www.kaggle.com/code/ambrosm/mabe-validated-baseline-without-machine-learning/\n",
    "    \n",
    "    Parameters:\n",
    "    dataset: (a subset of) the train dataframe\n",
    "    \n",
    "    Return values:\n",
    "    solution: solution dataframe in the correct format for the score() function\n",
    "    \"\"\"\n",
    "    solution = []\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "    \n",
    "        # Load annotation file\n",
    "        lab_id = row['lab_id']\n",
    "        if lab_id.startswith('MABe22'): continue\n",
    "        video_id = row['video_id']\n",
    "        path = f\"{cwd}/Data/train_annotation/{lab_id}/{video_id}.parquet\"\n",
    "        try:\n",
    "            annot = pd.read_parquet(path)\n",
    "        except FileNotFoundError:\n",
    "            # MABe22 and one more training file lack annotations.\n",
    "            if verbose: print(f\"No annotations for {path}\")\n",
    "            continue\n",
    "    \n",
    "        # Add all annotations to the solution\n",
    "        annot['lab_id'] = lab_id\n",
    "        annot['video_id'] = video_id\n",
    "        annot['behaviors_labeled'] = row['behaviors_labeled']\n",
    "        annot['target_id'] = np.where(annot.target_id != annot.agent_id, annot['target_id'].apply(lambda s: f\"mouse{s}\"), 'self')\n",
    "        annot['agent_id'] = annot['agent_id'].apply(lambda s: f\"mouse{s}\")\n",
    "        solution.append(annot)\n",
    "    \n",
    "    solution = pd.concat(solution)\n",
    "    return solution\n",
    "\n",
    "if validate_or_submit == 'validate':\n",
    "    solution = create_solution_df(train_without_mabe22)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Stress testing with unusual inputs\n",
    "\n",
    "After submission, this notebook will see a test set that it has never seen before. \n",
    "If the notebook crashes, debugging will be hard. \n",
    "It's better to stress-test the notebook before the submission by giving it some unusual inputs. \n",
    "The following hidden cell generate synthetic data with missing values, excessively long videos and so on.\n",
    "\"\"\"\n",
    "\n",
    "if validate_or_submit == 'stresstest':\n",
    "    n_videos_per_lab = 2\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"stresstest_tracking\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    stresstest = pd.concat(\n",
    "        [train.query(\"video_id == 1459695188\")] # long video from BoisterousParrot\n",
    "        + [df.sample(min(n_videos_per_lab, len(df)), random_state=1) for (_, df) in train.groupby('lab_id')])\n",
    "    for _, row in tqdm(stresstest.iterrows(), total=len(stresstest)):\n",
    "        lab_id = row['lab_id']\n",
    "        video_id = row['video_id']\n",
    "        \n",
    "        # Load video\n",
    "        path = f\"{cwd}/Data/train_tracking/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "    \n",
    "        if video_id == 1459695188: # long video from BoisterousParrot\n",
    "            vid = pd.concat([vid] * 3) # provoke out of memory (5 is too much)\n",
    "            vid['video_frame'] = np.arange(len(vid))\n",
    "    \n",
    "        # Drop some complete frames\n",
    "        dropped_frames = list(rng.choice(np.unique(vid.video_frame), size=100, replace=False))\n",
    "        vid = vid.query(\"~ video_frame.isin(@dropped_frames)\")\n",
    "        \n",
    "        # Drop a complete bodypart\n",
    "        if rng.uniform() < 0.2:\n",
    "            dropped_bodypart = rng.choice(np.unique(vid.bodypart), size=1, replace=False)[0]\n",
    "            vid = vid.query(\"bodypart != @dropped_bodypart\")\n",
    "        \n",
    "        # Drop a mouse\n",
    "        if rng.uniform() < 0.1:\n",
    "            vid = vid.query(\"mouse_id != 1\")\n",
    "        \n",
    "        # Drop random bodyparts from random frames\n",
    "        if rng.uniform() < 0.7:\n",
    "            mask = np.ones(len(vid), dtype=bool)\n",
    "            mask[:int(0.4 * len(mask))] = False\n",
    "            rng.shuffle(mask)\n",
    "            vid = vid[mask]\n",
    "    \n",
    "        # Set random coordinates of bodyparts to nan\n",
    "        if rng.uniform() < 0.7:\n",
    "            mask = np.ones(len(vid), dtype=bool)\n",
    "            mask[:int(0.2 * len(mask))] = False\n",
    "            rng.shuffle(mask)\n",
    "            vid.loc[:, 'x'] = np.where(mask, np.nan, vid.loc[:, 'x'])\n",
    "            rng.shuffle(mask)\n",
    "            vid.loc[:, 'y'] = np.where(mask, np.nan, vid.loc[:, 'y'])\n",
    "    \n",
    "        # Save the video\n",
    "        try:\n",
    "            os.mkdir(f\"stresstest_tracking/{lab_id}\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        new_path = f\"stresstest_tracking/{lab_id}/{video_id}.parquet\"\n",
    "        vid.to_parquet(new_path)\n",
    "\n",
    "'''\n",
    "# Challenge 1: Modeling for variable-sized sets of mice\n",
    "\n",
    "The first challenge we're going to solve is the fact that we have a variable number of mice (2, 3 or 4), \n",
    "and that the labeled behaviors apply either to one mouse or a pair of mice.\n",
    "\n",
    "The following function, `generate_mouse_data()`, solves this challenge. \n",
    "It transforms the dataset into batches. There are single-mouse batches and mouse-pair batches. \n",
    "Every single-mouse batch has data of only one mouse, every mouse-pair batch has data of exactly two mice. \n",
    "A single video frame can end up in several batches. \n",
    "If the frame has two visible mice, it can be part of four batches:\n",
    "- a single-mouse batch for individual behavior of mouse 1\n",
    "- a single-mouse batch for individual behavior of mouse 2\n",
    "- a mouse-pair batch for actions of mouse 1 with mouse 2 as target\n",
    "- a mouse-pair batch for actions of mouse 2 with mouse 1 as target\n",
    "\n",
    "The features (`data`) will consist of coordinates of body parts; the metadata (`meta`) \n",
    "will specify which mouse is / which mice are involved.\n",
    "'''\n",
    "\n",
    "drop_body_parts =  ['headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "                    'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "                    'spine_1', 'spine_2',\n",
    "                    'tail_middle_1', 'tail_middle_2', 'tail_midpoint']\n",
    "\n",
    "def generate_mouse_data(dataset, traintest, traintest_directory=None, generate_single=True, generate_pair=True, config: CTRGCNConfig | None = None):\n",
    "    \"\"\"Generate batches of data in coordinate representation.\n",
    "\n",
    "    The batches have variable length, and every batch can have other columns\n",
    "    for the labels, depending on what behaviors\n",
    "    were labeled for the batch.\n",
    "\n",
    "    Every video can produce zero, one or two batches.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: (subset of) train.csv or test.csv dataframe\n",
    "    traintest: either 'train' or 'test'\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    switch: either 'single' or 'pair'\n",
    "    data: dataframe containing coordinates of the body parts of a single mouse or of a pair of mice\n",
    "    meta: dataframe with columns ['video_id', 'agent_id', 'target_id', 'video_frame']\n",
    "    label: dataframe with labels (0, 1), one column per action, only if traintest == 'train'\n",
    "    actions: list of actions to be predicted for this batch, only if traintest == 'test'\n",
    "    \"\"\"\n",
    "    assert traintest in ['train', 'test']\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "    # CTR-GCN config limits allow dev/validate/submit modes to reduce data volume without changing default behavior.\n",
    "    video_count = 0\n",
    "    batch_count = 0\n",
    "    for _, row in dataset.iterrows():\n",
    "        if config is not None and config.max_videos is not None and video_count >= config.max_videos:\n",
    "            break\n",
    "        \n",
    "        # Load the video and pivot it sn that one frame = one row\n",
    "        lab_id = row.lab_id\n",
    "        if lab_id.startswith('MABe22'): continue\n",
    "        video_id = row.video_id\n",
    "\n",
    "        if type(row.behaviors_labeled) != str:\n",
    "            # We cannot use videos without labeled behaviors\n",
    "            print('No labeled behaviors:', lab_id, video_id, type(row.behaviors_labeled), row.behaviors_labeled)\n",
    "            continue\n",
    "\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid.query(\"~ bodypart.isin(@drop_body_parts)\")\n",
    "        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n",
    "        if pvid.isna().any().any():\n",
    "            if verbose and traintest == 'test': print('video with missing values', video_id, traintest, len(vid), 'frames')\n",
    "        else:\n",
    "            if verbose and traintest == 'test': print('video with all values', video_id, traintest, len(vid), 'frames')\n",
    "        del vid\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T # mouse_id, body_part, xy\n",
    "        pvid /= row.pix_per_cm_approx # convert to cm\n",
    "\n",
    "        # Determine the behaviors of this video\n",
    "        vid_behaviors = json.loads(row.behaviors_labeled)\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "        \n",
    "        # Load the annotations\n",
    "        if traintest == 'train':\n",
    "            try:\n",
    "                annot = pd.read_parquet(path.replace('train_tracking', 'train_annotation'))\n",
    "            except FileNotFoundError:\n",
    "                # MABe22 and one more training file lack annotations.\n",
    "                # We simply drop these videos.\n",
    "                continue\n",
    "\n",
    "        video_count += 1\n",
    "\n",
    "        # Create the single_mouse dataframes: single_mouse, single_mouse_label and single_mouse_meta\n",
    "        if generate_single:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\") # single-mouse behaviors of this video\n",
    "            for mouse_id_str in np.unique(vid_behaviors_subset.agent):\n",
    "                if config is not None and config.max_batches is not None and batch_count >= config.max_batches:\n",
    "                    return\n",
    "                try:\n",
    "                    mouse_id = int(mouse_id_str[-1])\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n",
    "                    single_mouse = pvid.loc[:, mouse_id]\n",
    "                    assert len(single_mouse) == len(pvid)\n",
    "                    single_mouse_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': mouse_id_str,\n",
    "                        'target_id': 'self',\n",
    "                        'video_frame': single_mouse.index\n",
    "                    })\n",
    "                    if traintest == 'train':\n",
    "                        single_mouse_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=single_mouse.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            single_mouse_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'single', single_mouse, single_mouse_meta, single_mouse_label\n",
    "                        batch_count += 1\n",
    "                    else:\n",
    "                        if verbose: print('- test single', video_id, mouse_id)\n",
    "                        yield 'single', single_mouse, single_mouse_meta, vid_agent_actions\n",
    "                        batch_count += 1\n",
    "                except KeyError:\n",
    "                    pass # If there is no data for the selected agent mouse, we skip the mouse.\n",
    "\n",
    "        # Create the mouse_pair dataframes: mouse_pair, mouse_label and mouse_meta\n",
    "        if generate_pair:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n",
    "            if len(vid_behaviors_subset) > 0:\n",
    "                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values('mouse_id')), 2): # int8\n",
    "                    if config is not None and config.max_batches is not None and batch_count >= config.max_batches:\n",
    "                        return\n",
    "                    agent_str = f\"mouse{agent}\"\n",
    "                    target_str = f\"mouse{target}\"\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n",
    "                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n",
    "                    assert len(mouse_pair) == len(pvid)\n",
    "                    mouse_pair_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': agent_str,\n",
    "                        'target_id': target_str,\n",
    "                        'video_frame': mouse_pair.index\n",
    "                    })\n",
    "                    if traintest == 'train':\n",
    "                        mouse_pair_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=mouse_pair.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            mouse_pair_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, mouse_pair_label\n",
    "                        batch_count += 1\n",
    "                    else:\n",
    "                        if verbose: print('- test pair', video_id, agent, target)\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, vid_agent_actions\n",
    "                        batch_count += 1\n",
    "\n",
    "\"\"\"\n",
    "# Challenge 2: Multiclass prediction with missing labels\n",
    "\n",
    "This competition is a multi-class classification task. \n",
    "For every video_id/video_frame/agent/target combination, we may predict at most one of several actions. \n",
    "Every action is a class, and 'no-action' is an additional class.\n",
    "\n",
    "We cannot use a standard multi-class estimator from scikit-learn \n",
    "because many values in the labels of our dataset are missing. \n",
    "For this reason, we train a binary classifier for every action, \n",
    "omitting the samples for which the target is unknown. \n",
    "Every binary classificator predicts a probability, \n",
    "and for the multiclass prediction we predict the class with the highest binary probability, \n",
    "if this probability is above a threshold; otherwise, we predict no action.\n",
    "\"\"\"\n",
    "\n",
    "# Make the multi-class prediction\n",
    "def predict_multiclass(pred, meta):\n",
    "    \"\"\"Derive multiclass predictions from a set of binary predictions.\n",
    "    \n",
    "    Parameters\n",
    "    pred: dataframe of predicted binary probabilities, shape (n_samples, n_actions), index doesn't matter\n",
    "    meta: dataframe with columns ['video_id', 'agent_id', 'target_id', 'video_frame'], index doesn't matter\n",
    "    \"\"\"\n",
    "    # Find the most probable class, but keep it only if its probability is above the threshold\n",
    "    threshold = 0.27\n",
    "    ama = np.argmax(pred, axis=1)\n",
    "    ama = np.where(pred.max(axis=1) >= threshold, ama, -1)\n",
    "    ama = pd.Series(ama, index=meta.video_frame)\n",
    "    # Keep only start and stop frames\n",
    "    changes_mask = (ama != ama.shift(1)).values\n",
    "    ama_changes = ama[changes_mask]\n",
    "    meta_changes = meta[changes_mask]\n",
    "    # mask selects the start frames\n",
    "    mask = ama_changes.values >= 0 # start of action\n",
    "    mask[-1] = False\n",
    "    submission_part = pd.DataFrame({\n",
    "        'video_id': meta_changes['video_id'][mask].values,\n",
    "        'agent_id': meta_changes['agent_id'][mask].values,\n",
    "        'target_id': meta_changes['target_id'][mask].values,\n",
    "        'action': pred.columns[ama_changes[mask].values],\n",
    "        'start_frame': ama_changes.index[mask],\n",
    "        'stop_frame': ama_changes.index[1:][mask[:-1]]\n",
    "    })\n",
    "    stop_video_id = meta_changes['video_id'][1:][mask[:-1]].values\n",
    "    stop_agent_id = meta_changes['agent_id'][1:][mask[:-1]].values\n",
    "    stop_target_id = meta_changes['target_id'][1:][mask[:-1]].values\n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "        if stop_video_id[i] != video_id or stop_agent_id[i] != agent_id or stop_target_id[i] != target_id:\n",
    "            new_stop_frame = meta.query(\"(video_id == @video_id)\").video_frame.max() + 1\n",
    "            submission_part.iat[i, submission_part.columns.get_loc('stop_frame')] = new_stop_frame\n",
    "    assert (submission_part.stop_frame > submission_part.start_frame).all(), 'stop <= start'\n",
    "    if verbose: print('  actions found:', len(submission_part))\n",
    "    return submission_part\n",
    "\n",
    "'''\n",
    "# Challenge 3: Transforming coordinates to an invariant representation\n",
    "\n",
    "The body part of the mice are given in cartesian coordinates. \n",
    "If the mice show some behavior at varying positions and with varying spatial orientation, \n",
    "cartesian coordinates are an inadequate representation. \n",
    "Our feature engineering transforms the coordinates to distances between body parts. \n",
    "Distances are invariant under translation and rotation.\n",
    "\n",
    "For a single mouse, the distances indicate whether and how much it turns its head, \n",
    "shoulders, hip and tail left or right. \n",
    "For a pair of mice, the distances indicate how far the head of the first mouse is near \n",
    "what part of the second one, and what body parts either mouse turns towards or away from the other one.\n",
    "'''\n",
    "\n",
    "def transform_single(single_mouse, body_parts_tracked):\n",
    "    \"\"\"Transform from cartesian coordinates to distance representation.\n",
    "\n",
    "    Parameters:\n",
    "    single_mouse: dataframe with coordinates of the body parts of one mouse\n",
    "                  shape (n_samples, n_body_parts * 2)\n",
    "                  two-level MultiIndex on columns\n",
    "    body_parts_tracked: list of body parts\n",
    "    \"\"\"\n",
    "    available_body_parts = single_mouse.columns.get_level_values(0)\n",
    "    X = pd.DataFrame({\n",
    "            f\"{part1}+{part2}\": np.square(single_mouse[part1] - single_mouse[part2]).sum(axis=1, skipna=False)\n",
    "            for part1, part2 in itertools.combinations(body_parts_tracked, 2) if part1 in available_body_parts and part2 in available_body_parts\n",
    "        })\n",
    "    X = X.reindex(columns=[f\"{part1}+{part2}\" for part1, part2 in itertools.combinations(body_parts_tracked, 2)], copy=False)\n",
    "\n",
    "    if 'ear_left' in single_mouse.columns and 'ear_right' in single_mouse.columns and 'tail_base' in single_mouse.columns:\n",
    "        shifted = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(10)\n",
    "        X = pd.concat([\n",
    "            X, \n",
    "            pd.DataFrame({\n",
    "                'speed_left': np.square(single_mouse['ear_left'] - shifted['ear_left']).sum(axis=1, skipna=False),\n",
    "                'speed_right': np.square(single_mouse['ear_right'] - shifted['ear_right']).sum(axis=1, skipna=False),\n",
    "                'speed_left2': np.square(single_mouse['ear_left'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "                'speed_right2': np.square(single_mouse['ear_right'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "            })\n",
    "        ], axis=1)\n",
    "    return X\n",
    "\n",
    "def transform_pair(mouse_pair, body_parts_tracked):\n",
    "    \"\"\"Transform from cartesian coordinates to distance representation.\n",
    "\n",
    "    Parameters:\n",
    "    mouse_pair: dataframe with coordinates of the body parts of two mice\n",
    "                  shape (n_samples, 2 * n_body_parts * 2)\n",
    "                  three-level MultiIndex on columns\n",
    "    body_parts_tracked: list of body parts\n",
    "    \"\"\"\n",
    "    # drop_body_parts =  ['ear_left', 'ear_right',\n",
    "    #                     'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "    #                     'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "    #                     'tail_midpoint']\n",
    "    # if len(body_parts_tracked) > 5:\n",
    "    #     body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "    available_body_parts_A = mouse_pair['A'].columns.get_level_values(0)\n",
    "    available_body_parts_B = mouse_pair['B'].columns.get_level_values(0)\n",
    "    X = pd.DataFrame({\n",
    "            f\"12+{part1}+{part2}\": np.square(mouse_pair['A'][part1] - mouse_pair['B'][part2]).sum(axis=1, skipna=False)\n",
    "            for part1, part2 in itertools.product(body_parts_tracked, repeat=2) if part1 in available_body_parts_A and part2 in available_body_parts_B\n",
    "        })\n",
    "    X = X.reindex(columns=[f\"12+{part1}+{part2}\" for part1, part2 in itertools.product(body_parts_tracked, repeat=2)], copy=False)\n",
    "\n",
    "    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n",
    "        shifted_A = mouse_pair['A']['ear_left'].shift(10)\n",
    "        shifted_B = mouse_pair['B']['ear_left'].shift(10)\n",
    "        X = pd.concat([\n",
    "            X,\n",
    "            pd.DataFrame({\n",
    "                'speed_left_A': np.square(mouse_pair['A']['ear_left'] - shifted_A).sum(axis=1, skipna=False),\n",
    "                'speed_left_AB': np.square(mouse_pair['A']['ear_left'] - shifted_B).sum(axis=1, skipna=False),\n",
    "                'speed_left_B': np.square(mouse_pair['B']['ear_left'] - shifted_B).sum(axis=1, skipna=False),\n",
    "            })\n",
    "        ], axis=1)\n",
    "    return X\n",
    "\n",
    "'''\n",
    "# Cross-validation\n",
    "\n",
    "We're now almost ready to cross-validate our models. \n",
    "\n",
    "The following function gets as input\n",
    "- a binary classification model\n",
    "- a 2d array of features (i.e., distances between body parts); after we have dealt with variable-sized mouse sets (challenge 1) and variable-sized bodyparts sets (challenge 5), this array is rectangular.\n",
    "- a 2d array of binary labels, some elements of which may be missing\n",
    "- a 2d array of metadata so that we can match the predictions with the original video_id, agent, target and video_frame\n",
    "\n",
    "It first computes out-of-fold predictions with a set of binary classifiers \n",
    "and then transforms these binary predictions into a multiclass prediction (see above).\n",
    "'''\n",
    "\n",
    "threshold = 0.27\n",
    "f1_list = []\n",
    "def cross_validate_classifier(binary_classifier, X, label, meta):\n",
    "    \"\"\"Cross-validate a binary classifier per action and a multi-class classifier over all actions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    binary_classifier: classifier with predict_proba\n",
    "    X: 2d array-like (distance representation) of shape (n_samples, n_features)\n",
    "    label: dataframe with binary targets (one column per action, may have missing values), index doesn't matter\n",
    "    meta: dataframe with columns ['video_id', 'agent_id', 'target_id', 'video_frame'], index doesn't matter\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    appends to f1_list (binary) and submission_list (multi-class)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Cross-validate a binary classifier for every action\n",
    "    oof = pd.DataFrame(index=meta.video_frame) # will get a column per action\n",
    "    for action in label.columns:\n",
    "        # Filter for samples (video frames) with a defined target (i.e., target is not nan)\n",
    "        action_mask = ~ label[action].isna().values\n",
    "        X_action = X[action_mask]\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "        p = y_action.mean()\n",
    "        baseline_score = p / (1 + p)\n",
    "        groups_action = meta.video_id[action_mask] # ensure validation has unseen videos\n",
    "        if len(np.unique(groups_action)) < 5:\n",
    "            continue # GroupKFold would fail with fewer than n_splits groups\n",
    "\n",
    "        if not (y_action == 0).all():\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "                # Number of classes in training fold (1) does not match total number of classes (2)\n",
    "                oof_action = cross_val_predict(binary_classifier, X_action, y_action, groups=groups_action, cv=GroupKFold(), method='predict_proba')\n",
    "            oof_action = oof_action[:, 1]\n",
    "        else:\n",
    "            oof_action = np.zeros(len(y_action))\n",
    "        f1 = f1_score(y_action, (oof_action >= threshold), zero_division=0)\n",
    "        ch = '>' if f1 > baseline_score else '=' if f1 == baseline_score else '<'\n",
    "        print(f\"  F1: {f1:.3f} {ch} ({baseline_score:.3f}) {action}\")\n",
    "        f1_list.append((body_parts_tracked_str, action, f1)) # type: ignore\n",
    "        oof_column = np.zeros(len(label))\n",
    "        oof_column[action_mask] = oof_action\n",
    "        oof[action] = oof_column\n",
    "\n",
    "    # Make the multi-class prediction\n",
    "    submission_part = predict_multiclass(oof, meta)\n",
    "    submission_list.append(submission_part) # type: ignore\n",
    "\n",
    "\n",
    "def load_ctr_gcn_models(\n",
    "    model_dir: str,\n",
    "    actions: list[str],\n",
    "    adjacency: np.ndarray,\n",
    "    config: CTRGCNConfig,\n",
    "    device: str = \"cpu\",\n",
    ") -> dict[str, nn.Module]:\n",
    "    \"\"\"\n",
    "    Load trained CTR-GCN models from disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_dir : str\n",
    "        Directory containing \"{action}.pt\" weight files.\n",
    "    actions : list[str]\n",
    "        Action names to load models for.\n",
    "    adjacency : np.ndarray\n",
    "        Adjacency matrix (V,V) for the joints.\n",
    "    config : CTRGCNConfig\n",
    "        Configuration specifying stream_mode and channel sizes.\n",
    "    device : str\n",
    "        \"cpu\", \"cuda\", or \"mps\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_dict : dict[str, nn.Module]\n",
    "        Loaded models, all in eval mode and moved to the correct device.\n",
    "    \"\"\"\n",
    "    model_dict: dict[str, nn.Module] = {}\n",
    "\n",
    "    mode = getattr(config, \"stream_mode\", \"one\")\n",
    "    if getattr(config, \"two_stream\", False) and mode == \"one\":\n",
    "        mode = \"two\"\n",
    "\n",
    "    for action in actions:\n",
    "        if mode == \"one\":\n",
    "            model = CTRGCNMinimal(\n",
    "                in_channels=config.in_channels_single_stream,\n",
    "                num_classes=1,\n",
    "                adjacency=adjacency,\n",
    "            )\n",
    "        elif mode == \"two\":\n",
    "            model = CTRGCNTwoStream(\n",
    "                adjacency=adjacency,\n",
    "                in_channels_coords=config.in_channels_streamA,\n",
    "                in_channels_delta=config.in_channels_streamB,\n",
    "            )\n",
    "        else:\n",
    "            model = CTRGCNFourStream(adjacency=adjacency)\n",
    "\n",
    "        weight_path = os.path.join(model_dir, f\"{action}.pt\")\n",
    "        state = torch.load(weight_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        model_dict[action] = model\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def submit_ctr_gcn(\n",
    "    body_parts_tracked_str: str,\n",
    "    switch_tr: str,\n",
    "    model_dict: dict[str, nn.Module],\n",
    "    config: CTRGCNConfig,\n",
    "    device: str = \"cpu\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate submission parts using pre-trained CTR-GCN models\n",
    "    (inference-only, no training). Appends submission_part DataFrames\n",
    "    to the global submission_list, exactly like the LightGBM submit().\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body_parts_tracked_str : str\n",
    "        JSON list of body parts tracked.\n",
    "    switch_tr : str\n",
    "        \"single\" or \"pair\".\n",
    "    model_dict : dict[str, nn.Module]\n",
    "        Maps action → pretrained CTR-GCN model.\n",
    "    config : CTRGCNConfig\n",
    "        Contains stream_mode, use_delta, bone flags, etc.\n",
    "    device : str\n",
    "        \"cpu\", \"cuda\", or \"mps\".\n",
    "    \"\"\"\n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "\n",
    "    if validate_or_submit == \"submit\":\n",
    "        test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "        generator = generate_mouse_data(\n",
    "            test_subset,\n",
    "            \"test\",\n",
    "            generate_single=(switch_tr == \"single\"),\n",
    "            generate_pair=(switch_tr == \"pair\"),\n",
    "            config=config,\n",
    "        )\n",
    "    else:\n",
    "        test_subset = stresstest.query(\"body_parts_tracked == @body_parts_tracked_str\")\n",
    "        generator = generate_mouse_data(\n",
    "            test_subset,\n",
    "            \"test\",\n",
    "            traintest_directory=\"stresstest_tracking\",\n",
    "            generate_single=(switch_tr == \"single\"),\n",
    "            generate_pair=(switch_tr == \"pair\"),\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"n_videos: {len(test_subset)}\")\n",
    "\n",
    "    ordered_joints, adjacency = get_ordered_joints_and_adjacency(body_parts_tracked)\n",
    "\n",
    "    mode = getattr(config, \"stream_mode\", \"one\")\n",
    "    if getattr(config, \"two_stream\", False) and mode == \"one\":\n",
    "        mode = \"two\"\n",
    "\n",
    "    for switch_te, data_te, meta_te, actions_te in generator:\n",
    "        assert switch_te == switch_tr\n",
    "\n",
    "        actions_available = [a for a in actions_te if a in model_dict]\n",
    "        if not actions_available:\n",
    "            if verbose:\n",
    "                print(f\"  No CTR-GCN models for actions {actions_te}\")\n",
    "            continue\n",
    "\n",
    "        if mode == \"one\":\n",
    "            window_tensor, frame_ranges = prepare_ctr_gcn_input(data_te, ordered_joints, config)\n",
    "            if window_tensor.shape[0] == 0:\n",
    "                continue\n",
    "            X = window_tensor.to(device)\n",
    "        elif mode == \"two\":\n",
    "            streamA_list, streamB_list, frame_ranges = prepare_ctr_gcn_input(data_te, ordered_joints, config)\n",
    "            if len(streamA_list) == 0:\n",
    "                continue\n",
    "            X_streamA = torch.stack(streamA_list, dim=0).to(device)\n",
    "            X_streamB = torch.stack(streamB_list, dim=0).to(device)\n",
    "        else:\n",
    "            coords_list, delta_list, bone_list, bone_delta_list, frame_ranges = prepare_ctr_gcn_input(data_te, ordered_joints, config)\n",
    "            if len(coords_list) == 0:\n",
    "                continue\n",
    "            X_coords = torch.stack(coords_list, dim=0).to(device)\n",
    "            X_delta = torch.stack(delta_list, dim=0).to(device)\n",
    "            X_bone = torch.stack(bone_list, dim=0).to(device)\n",
    "            X_bone_delta = torch.stack(bone_delta_list, dim=0).to(device)\n",
    "\n",
    "        if verbose and len(frame_ranges) == 0:\n",
    "            print(\"  No frame ranges produced.\")\n",
    "            continue\n",
    "\n",
    "        frame_values = meta_te.video_frame.values\n",
    "        frame_to_idx = {f: i for i, f in enumerate(frame_values)}\n",
    "        n_frames = len(frame_values)\n",
    "        n_actions = len(actions_available)\n",
    "\n",
    "        sum_probs = np.zeros((n_frames, n_actions), dtype=np.float32)\n",
    "        counts = np.zeros((n_frames, n_actions), dtype=np.float32)\n",
    "\n",
    "        for action_idx, action_name in enumerate(actions_available):\n",
    "            model = model_dict[action_name]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if mode == \"one\":\n",
    "                    logits = model(X)\n",
    "                elif mode == \"two\":\n",
    "                    logits = model(X_streamA, X_streamB)\n",
    "                else:\n",
    "                    logits = model(X_coords, X_delta, X_bone, X_bone_delta)\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "\n",
    "            for w_idx, frames in enumerate(frame_ranges):\n",
    "                p = float(probs[w_idx])\n",
    "                for f in frames:\n",
    "                    fi = frame_to_idx.get(f)\n",
    "                    if fi is None:\n",
    "                        continue\n",
    "                    sum_probs[fi, action_idx] += p\n",
    "                    counts[fi, action_idx] += 1.0\n",
    "\n",
    "        counts_safe = counts.copy()\n",
    "        counts_safe[counts_safe == 0] = 1.0\n",
    "        pred_array = sum_probs / counts_safe\n",
    "\n",
    "        pred_df = pd.DataFrame(\n",
    "            pred_array,\n",
    "            index=meta_te.video_frame,\n",
    "            columns=actions_available,\n",
    "        )\n",
    "\n",
    "        submission_part = predict_multiclass(pred_df, meta_te)\n",
    "        submission_list.append(submission_part)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# =============================\n",
    "# EXAMPLE: TRAIN → SAVE WEIGHTS\n",
    "# =============================\n",
    "# config = CTRGCNConfig(mode=\"validate\")\n",
    "# batches = [...]  # from generate_mouse_data\n",
    "# ordered_joints, adjacency = get_ordered_joints_and_adjacency(body_parts_tracked)\n",
    "# model_dict = train_ctr_gcn_models(batches, ordered_joints, adjacency, config)\n",
    "\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "# for action, model in model_dict.items():\n",
    "#     torch.save(model.state_dict(), f\"models/{action}.pt\")\n",
    "\n",
    "# =============================\n",
    "# EXAMPLE: LOAD WEIGHTS → INFERENCE\n",
    "# =============================\n",
    "# loaded_models = load_ctr_gcn_models(\n",
    "#     \"models/\",\n",
    "#     actions=list(model_dict.keys()),\n",
    "#     adjacency=adjacency,\n",
    "#     config=config,\n",
    "#     device=\"cpu\",\n",
    "# )\n",
    "\n",
    "# submit_ctr_gcn(\n",
    "#     body_parts_tracked_str,\n",
    "#     switch_tr,\n",
    "#     loaded_models,\n",
    "#     config,\n",
    "#     device=\"cpu\",\n",
    "# )\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "# Challenge 4: A dataset that doesn't fit into memory\n",
    "\n",
    "The competition dataset doesn't fit into memory as whole. \n",
    "The problem is exacerbated if we compute lots of distance in feature engineering. \n",
    "We tackle this challenge with the following measures:\n",
    "- Training on a subset of the data: The training dataset is highly redundant. In videos taken with 30 frames per second, the difference from one frame to the next is small. We can well afford to subsample the training data.\n",
    "- Processing the test data in batches: There is no need to have the full test dataset in memory at any time. (This decision has the drawback that the test data are read from disk several times.)\n",
    "- It helps that we split all data by body_parts_tracked (see challenge 5 below). This way, we don't even need to have the full training dataset in memory.\n",
    "'''\n",
    "\n",
    "def submit(body_parts_tracked_str, switch_tr, binary_classifier, X_tr, label, meta):\n",
    "    \"\"\"Produce a submission file for the selected subset of the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body_parts_tracked_str: subset of body parts for filtering the test set\n",
    "    switch_tr: 'single' or 'pair'\n",
    "    binary_classifier: classifier with predict_proba\n",
    "    X_tr: training features as 2d array-like of shape (n_samples, n_features)\n",
    "    label: dataframe with binary targets (one column per action, may have missing values), index doesn't matter\n",
    "    meta: dataframe with columns ['video_id', 'agent_id', 'target_id', 'video_frame'], index doesn't matter\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    appends to submission_list\n",
    "    \n",
    "    \"\"\"\n",
    "    # Fit a binary classifier for every action\n",
    "    model_list = [] # will get a model per action\n",
    "    for action in label.columns:\n",
    "        # Filter for samples (video frames) with a defined target (i.e., target is not nan)\n",
    "        action_mask = ~ label[action].isna().values\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "\n",
    "        if not (y_action == 0).all():\n",
    "            model = clone(binary_classifier)\n",
    "            model.fit(X_tr[action_mask], y_action)\n",
    "            assert len(model.classes_) == 2\n",
    "            model_list.append((action, model))\n",
    "\n",
    "    # Compute test predictions in batches\n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    if len(body_parts_tracked) > 5:\n",
    "        body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "    if validate_or_submit == 'submit':\n",
    "        test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "        generator = generate_mouse_data(test_subset, 'test',\n",
    "                                        generate_single=(switch_tr == 'single'), \n",
    "                                        generate_pair=(switch_tr == 'pair'))\n",
    "    else:\n",
    "        test_subset = stresstest.query(\"body_parts_tracked == @body_parts_tracked_str\")\n",
    "        generator = generate_mouse_data(test_subset, 'test',\n",
    "                                        traintest_directory='stresstest_tracking',\n",
    "                                        generate_single=(switch_tr == 'single'),\n",
    "                                        generate_pair=(switch_tr == 'pair'))\n",
    "    if verbose: print(f\"n_videos: {len(test_subset)}\")\n",
    "    for switch_te, data_te, meta_te, actions_te in generator:\n",
    "        assert switch_te == switch_tr\n",
    "        try:\n",
    "            # Transform from coordinate representation into distance representation\n",
    "            if switch_te == 'single':\n",
    "                X_te = transform_single(data_te, body_parts_tracked) # may raise KeyError\n",
    "            else:\n",
    "                X_te = transform_pair(data_te, body_parts_tracked) # may raise KeyError\n",
    "            if verbose and len(X_te) == 0: print(\"ERROR: X_te is empty\")\n",
    "            del data_te\n",
    "    \n",
    "            # Compute binary predictions\n",
    "            pred = pd.DataFrame(index=meta_te.video_frame) # will get a column per action\n",
    "            for action, model in model_list:\n",
    "                if action in actions_te:\n",
    "                    pred[action] = model.predict_proba(X_te)[:, 1]\n",
    "            del X_te\n",
    "            # Compute multiclass predictions\n",
    "            if pred.shape[1] != 0:\n",
    "                submission_part = predict_multiclass(pred, meta_te)\n",
    "                submission_list.append(submission_part) # type: ignore\n",
    "            else: # this happens if there was no useful training data for the test actions\n",
    "                if verbose: print(f\"  ERROR: no useful training data\")\n",
    "        except KeyError:\n",
    "            if verbose: print(f'  ERROR: KeyError because of missing bodypart ({switch_tr})')\n",
    "            del data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Validation Helper (External Workflow)\n",
    "#\n",
    "# This notebook is designed so that heavy CTR-GCN training is done\n",
    "# externally (e.g., on a local machine or HPC cluster). After\n",
    "# training, you can compute validation scores using the official\n",
    "# competition metric as follows:\n",
    "#\n",
    "#   1. Build a solution dataframe from a subset of the training set:\n",
    "#        solution = create_solution_df(train_without_mabe22)\n",
    "#\n",
    "#   2. Run your trained CTR-GCN models on the same subset to produce\n",
    "#      a submission-style dataframe of predictions:\n",
    "#        # Use your own inference loop or adapt submit_ctr_gcn\n",
    "#        # to write predictions for the chosen subset.\n",
    "#\n",
    "#   3. Evaluate with:\n",
    "#        score_value = score(solution, submission, row_id_column_name=\"row_id\")\n",
    "#        print(\"mouse_fbeta:\", score_value)\n",
    "#\n",
    "# The exact wiring of external training + validation will depend on\n",
    "# how you organize your CTR-GCN training script, but this cell provides\n",
    "# the scoring building blocks needed to compute the Kaggle metric.\n",
    "\n",
    "print(\"Validation helper and scoring utilities are ready (mouse_fbeta, score).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Inference-Only Submission Pipeline (RUN_MODE == 'submit')\n",
    "#\n",
    "# This cell expects that you have pre-trained CTR-GCN weights stored as\n",
    "# one file per action in a Kaggle dataset, for example:\n",
    "#\n",
    "#   /kaggle/input/mabe-gcn-models/\n",
    "#       attack.pt\n",
    "#       mount.pt\n",
    "#       ...\n",
    "#\n",
    "# The notebook will:\n",
    "#   - load the trained weights\n",
    "#   - iterate over all body_parts_tracked configurations\n",
    "#   - run CTR-GCN inference on the test set\n",
    "#   - aggregate predictions into a single submission.csv\n",
    "\n",
    "if RUN_MODE == \"submit\":\n",
    "    # Directory containing pre-trained CTR-GCN weights (one .pt per action).\n",
    "    MODEL_DIR = \"/kaggle/input/mabe-gcn-models\"\n",
    "    model_dir_path = Path(MODEL_DIR)\n",
    "    assert model_dir_path.exists(), f\"MODEL_DIR not found: {MODEL_DIR}\"\n",
    "\n",
    "    # Discover which actions we have weights for, based on file names.\n",
    "    actions = sorted([p.stem for p in model_dir_path.glob(\"*.pt\")])\n",
    "    print(\"Found actions with pretrained models:\", actions)\n",
    "\n",
    "    # Use the same body_parts_tracked_list and test dataframe defined above.\n",
    "    submission_list = []\n",
    "\n",
    "    # Skip index 0 (MABe22) as in the original notebook.\n",
    "    for section in range(1, len(body_parts_tracked_list)):\n",
    "        body_parts_tracked_str = body_parts_tracked_list[section]\n",
    "        try:\n",
    "            body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "        except Exception:\n",
    "            print(f\"Skipping malformed body_parts_tracked_str: {body_parts_tracked_str}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Section {section}: Processing videos with body parts {body_parts_tracked}\")\n",
    "\n",
    "        # Filter test rows with the matching body_parts_tracked.\n",
    "        test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "        if len(test_subset) == 0:\n",
    "            print(\"  No test videos for this configuration; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Build the anatomical ordering and adjacency for this body-part set.\n",
    "        ordered_joints, adjacency = get_ordered_joints_and_adjacency(body_parts_tracked)\n",
    "\n",
    "        # Configure CTR-GCN. Adjust stream_mode / flags if you trained multi-stream models.\n",
    "        config = CTRGCNConfig(\n",
    "            mode=\"submit\",\n",
    "            max_videos=None,\n",
    "            max_batches=None,\n",
    "            max_windows=None,\n",
    "            use_delta=True,\n",
    "            two_stream=False,\n",
    "            use_bone=True,\n",
    "            use_bone_delta=True,\n",
    "            stream_mode=\"one\",\n",
    "        )\n",
    "\n",
    "        # Load the pre-trained models for this adjacency configuration.\n",
    "        model_dict = load_ctr_gcn_models(\n",
    "            model_dir=MODEL_DIR,\n",
    "            actions=actions,\n",
    "            adjacency=adjacency,\n",
    "            config=config,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Run inference for single-mouse and mouse-pair batches.\n",
    "        submit_ctr_gcn(\n",
    "            body_parts_tracked_str=body_parts_tracked_str,\n",
    "            switch_tr=\"single\",\n",
    "            model_dict=model_dict,\n",
    "            config=config,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        submit_ctr_gcn(\n",
    "            body_parts_tracked_str=body_parts_tracked_str,\n",
    "            switch_tr=\"pair\",\n",
    "            model_dict=model_dict,\n",
    "            config=config,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    # Concatenate all partial submissions and write the final submission file.\n",
    "    if len(submission_list) == 0:\n",
    "        raise RuntimeError(\"No predictions were generated. Check MODEL_DIR and RUN_MODE.\")\n",
    "\n",
    "    submission = pd.concat(submission_list, ignore_index=True)\n",
    "    submission = submission.sort_values(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Saved submission.csv with shape:\", submission.shape)\n",
    "else:\n",
    "    print(\"RUN_MODE is not 'submit'; skipping inference-only submission pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88423a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEV-MODE SANITY CHECK\n",
    "# Runs a tiny end-to-end GCN pipeline test without real weights.\n",
    "# ============================================================\n",
    "\n",
    "if RUN_MODE == \"dev\":\n",
    "    print(\"\\n========== DEV MODE SANITY CHECK ==========\\n\")\n",
    "\n",
    "    # Pick a very small subset of train.csv\n",
    "    small_train = train_without_mabe22.sample(1, random_state=0)\n",
    "    print(\"Selected video:\", small_train.video_id.values[0])\n",
    "\n",
    "    # Extract body-parts config\n",
    "    body_parts_tracked_str = small_train.body_parts_tracked.values[0]\n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    print(\"Body parts for this dev test:\", body_parts_tracked)\n",
    "\n",
    "    # Build joints + adjacency\n",
    "    ordered_joints, adjacency = get_ordered_joints_and_adjacency(body_parts_tracked)\n",
    "    print(\"Ordered joints:\", ordered_joints)\n",
    "    print(\"Adjacency shape:\", adjacency.shape)\n",
    "\n",
    "    # Set a small config to avoid heavy processing\n",
    "    dev_config = CTRGCNConfig(\n",
    "        mode=\"dev\",\n",
    "        max_videos=1,\n",
    "        max_batches=2,\n",
    "        max_windows=3,\n",
    "        stream_mode=\"one\",\n",
    "        use_delta=True,\n",
    "        use_bone=True,\n",
    "        use_bone_delta=True,\n",
    "    )\n",
    "\n",
    "    # Use generate_mouse_data to get a few single-mouse batches\n",
    "    generator = generate_mouse_data(\n",
    "        small_train,\n",
    "        traintest=\"train\",\n",
    "        generate_single=True,\n",
    "        generate_pair=False,\n",
    "        config=dev_config\n",
    "    )\n",
    "\n",
    "    windows_collected = 0\n",
    "\n",
    "    for switch, data_df, meta_df, label_df in generator:\n",
    "        print(f\"\\nBatch type: {switch}\")\n",
    "        print(\"Data shape:\", data_df.shape)\n",
    "\n",
    "        # Build GCN inputs\n",
    "        X, frame_ranges = prepare_ctr_gcn_input(data_df, ordered_joints, dev_config)\n",
    "        print(\"Window tensor shape:\", X.shape)\n",
    "        print(\"Number of frame ranges:\", len(frame_ranges))\n",
    "\n",
    "        # Pick a random action column to test\n",
    "        test_actions = list(label_df.columns)\n",
    "        if len(test_actions) == 0:\n",
    "            print(\"No actions labeled in this tiny batch, skipping.\")\n",
    "            continue\n",
    "\n",
    "        action = test_actions[0]\n",
    "        print(\"Testing action:\", action)\n",
    "\n",
    "        # Dummy untrained model\n",
    "        in_channels = X.shape[1]\n",
    "        dummy_model = CTRGCNMinimal(\n",
    "            in_channels=in_channels,\n",
    "            num_classes=1,\n",
    "            adjacency=adjacency\n",
    "        ).to(device)\n",
    "\n",
    "        # Run forward pass\n",
    "        with torch.no_grad():\n",
    "            logits = dummy_model(X.to(device))\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "\n",
    "        print(\"Logits:\", logits[:5].cpu().numpy())\n",
    "        print(\"Probs:\", probs[:5])\n",
    "\n",
    "        # Convert to per-frame predictions (window averaging)\n",
    "        frame_values = meta_df.video_frame.values\n",
    "        frame_to_idx = {f: i for i, f in enumerate(frame_values)}\n",
    "        pred_array = np.zeros((len(frame_values), 1), dtype=np.float32)\n",
    "        counts = np.zeros((len(frame_values), 1), dtype=np.float32)\n",
    "\n",
    "        for w_idx, frames in enumerate(frame_ranges):\n",
    "            p = float(probs[w_idx])\n",
    "            for f in frames:\n",
    "                fi = frame_to_idx.get(f)\n",
    "                if fi is None:\n",
    "                    continue\n",
    "                pred_array[fi, 0] += p\n",
    "                counts[fi, 0] += 1.0\n",
    "\n",
    "        counts[counts == 0] = 1\n",
    "        pred_array = pred_array / counts\n",
    "\n",
    "        # Build a mini prediction df for predict_multiclass\n",
    "        pred_df = pd.DataFrame(pred_array, index=frame_values, columns=[action])\n",
    "        submission_part = predict_multiclass(pred_df, meta_df)\n",
    "\n",
    "        print(\"\\nGenerated mini submission_part:\")\n",
    "        print(submission_part.head())\n",
    "\n",
    "        windows_collected += 1\n",
    "        if windows_collected >= 2:\n",
    "            break\n",
    "\n",
    "    print(\"\\n========== DEV TEST COMPLETE ==========\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"RUN_MODE is not 'dev'; skip quick sanity test.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
