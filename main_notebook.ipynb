{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c01c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e94624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mouse_sequences():\n",
    "    # Define the input paths to read from and output paths to read to\n",
    "    annotated_dir=Path(\"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\")\n",
    "    tracking_dir=Path(\"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\")\n",
    "    annotated_output=Path(\"kaggle/working/annotated\")\n",
    "    tracking_output=Path(\"kaggle/working/tracking\")\n",
    "    annotated_output.mkdir(parents=True, exist_ok=True)\n",
    "    tracking_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Recursively list all the parquet files \n",
    "    tracking_files = list(tracking_dir.rglob(\"*.parquet\"))\n",
    "    annotation_files = list(annotated_dir.rglob(\"*.parquet\"))\n",
    "    # Creates a dictionary of the annotated files with the name as the key and the Path as the value\n",
    "    annotation_map = {f.name: f for f in annotation_files} \n",
    "    # Collect all metadata columns first\n",
    "    all_columns = set([\n",
    "        \"video_frame\", \"mouse_id\", \"action\", \"agent_id\", \"target_id\"\n",
    "    ])\n",
    "    # Create a set for each unique bodypart seen\n",
    "    bodyparts_seen = set()\n",
    "    # loop over every tracking file and fill the body parts set with the unique entries from the \"bodypart\" column\n",
    "    for track_file in tqdm(tracking_files, desc=\"Detecting bodyparts\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(track_file)\n",
    "            if \"bodypart\" not in df.columns or df[\"bodypart\"].empty:\n",
    "                tqdm.write(f\"No bodypart column present in {track_file}\")\n",
    "                continue\n",
    "            bodyparts_seen.update(df[\"bodypart\"].unique())\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error reading {track_file}: {e}\")\n",
    "    # Add x/y columns for each bodypart to the all columns set\n",
    "    for bp in bodyparts_seen:\n",
    "        all_columns.add(f\"{bp}_x\")\n",
    "        all_columns.add(f\"{bp}_y\")\n",
    "\n",
    "    # Loop over the tracking files a second time, this time for information merging and formatting\n",
    "    for track_file in tqdm(tracking_files, desc=\"Processing videos\"):\n",
    "        video_id = track_file.stem\n",
    "        tracking_df = pd.read_parquet(track_file).sort_values(\"video_frame\").reset_index(drop=True)\n",
    "        tracking_df[\"action\"] = None\n",
    "        tracking_df[\"agent_id\"] = None\n",
    "        tracking_df[\"target_id\"] = None\n",
    "        # Apply annotations if available\n",
    "        annot_file = annotation_map.get(track_file.name)\n",
    "        is_annotated = False # Flag for sorting the labelled VS unlabelled files in the output\n",
    "        if annot_file and annot_file.exists():\n",
    "            annotation_df = pd.read_parquet(annot_file)\n",
    "            for _, row in annotation_df.iterrows():\n",
    "                mask = (tracking_df.video_frame >= row.start_frame) & (tracking_df.video_frame <= row.stop_frame)\n",
    "                tracking_df.loc[mask, \"action\"] = row.action\n",
    "                tracking_df.loc[mask, \"agent_id\"] = row.agent_id\n",
    "                tracking_df.loc[mask, \"target_id\"] = row.target_id\n",
    "            is_annotated = True # Change the flag to indicate the tracking data is labelled \n",
    "\n",
    "        try:\n",
    "            # Check required columns\n",
    "            required_index = [\"video_frame\", \"mouse_id\",]\n",
    "            required_values = [\"x\", \"y\"]\n",
    "            missing_index_cols = [c for c in required_index if c not in tracking_df.columns]\n",
    "            missing_value_cols = [c for c in required_values + [\"bodypart\"] if c not in tracking_df.columns]\n",
    "\n",
    "            if missing_index_cols or missing_value_cols:\n",
    "                tqdm.write(f\"Missing columns in {track_file}:\")\n",
    "                if missing_index_cols:\n",
    "                    tqdm.write(f\"  Index columns missing: {missing_index_cols}\")\n",
    "                if missing_value_cols:\n",
    "                    tqdm.write(f\"  Value/bodypart columns missing: {missing_value_cols}\")\n",
    "                tqdm.write(\"Cannot pivot this file. Exiting to troubleshoot.\")\n",
    "                sys.exit(1)\n",
    "\n",
    "            # Pivot\n",
    "            pivoted = tracking_df.pivot_table(\n",
    "                index=required_index,\n",
    "                columns=\"bodypart\",\n",
    "                values=[\"x\", \"y\"],\n",
    "                aggfunc=\"first\"\n",
    "            )\n",
    "\n",
    "            # Check if pivot is empty\n",
    "            if pivoted.empty:\n",
    "                tqdm.write(f\"Pivot resulted in empty DataFrame for {track_file}.\")\n",
    "                tqdm.write(\"Tracking DF info:\")\n",
    "                tqdm.write(f\"  Shape: {tracking_df.shape}\")\n",
    "                tqdm.write(f\"  Columns: {tracking_df.columns.tolist()}\")\n",
    "                tqdm.write(f\"  Head:\\n{tracking_df.head()}\")\n",
    "                sys.exit(1)  # exit to troubleshoot\n",
    "\n",
    "            # Flatten multiindex columns\n",
    "            pivoted.columns = [f\"{bp}_{coord}\" for coord, bp in pivoted.columns]\n",
    "            pivoted = pivoted.reset_index()\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error pivoting {track_file}: {e}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Ensure uniform columns\n",
    "        missing_cols = all_columns - set(pivoted.columns)\n",
    "        for c in missing_cols:\n",
    "            pivoted[c] = pd.NA\n",
    "        pivoted = pivoted[list(all_columns)]  # enforce column order\n",
    "\n",
    "\n",
    "        if is_annotated:\n",
    "            output_dir = annotated_output\n",
    "        else:\n",
    "            output_dir = tracking_output\n",
    "        # Save, overwrite if exists\n",
    "        out_path = Path(output_dir) / f\"{video_id}.parquet\"\n",
    "        try:\n",
    "            pivoted.to_parquet(out_path, index=False)\n",
    "            print(f\"✅ Saved {out_path} with shape {pivoted.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not save {out_path}: {e}\")\n",
    "\n",
    "    print(f\"\\n✅ All processed files saved to {output_dir}\")\n",
    "\n",
    "\n",
    "(save_mouse_sequences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code snippet to get a sample structure of the two different data-sets\n",
    "'''\n",
    "\n",
    "annotated_example_path = Path(\"/kaggle/input/MABe-mouse-behavior-detection/train_annotation/CRIM13/1763467574.parquet\")\n",
    "tracking_example_path = Path(\"/kaggle/input/MABe-mouse-behavior-detection/train_tracking/CRIM13/1763467574.parquet\")\n",
    "\n",
    "labeled_df = pd.read_parquet(annotated_example_path)\n",
    "print(labeled_df.head(20),\"\\n\",\"\\n\")\n",
    "track_df = pd.read_parquet(tracking_example_path)\n",
    "print(track_df.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
